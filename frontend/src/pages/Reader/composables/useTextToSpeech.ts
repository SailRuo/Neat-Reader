import { ref, computed, onMounted, onUnmounted } from 'vue'
import * as ttsAPI from '@/api/tts'

export type TTSEngine = 'browser' | 'edge' | 'piper'

export interface TTSOptions {
    lang?: string
    rate?: number
    volume?: number
    pitch?: number
    engine?: TTSEngine
    voice?: string
}

// éŸ³é¢‘ç‰‡æ®µç¼“å­˜
interface AudioSegment {
    text: string
    audioBlob?: Blob
    audioUrl?: string
    isLoading: boolean
    error?: string
}

export function useTextToSpeech() {
    const isPlaying = ref(false)
    const isPaused = ref(false)
    const voices = ref<SpeechSynthesisVoice[]>([])
    const edgeVoices = ref<ttsAPI.TTSVoice[]>([])
    const selectedVoice = ref<SpeechSynthesisVoice | null>(null)
    const selectedEdgeVoice = ref<string>('zh-CN-XiaoxiaoNeural')
    const engine = ref<TTSEngine>('edge') // é»˜è®¤ä½¿ç”¨ Edge TTS
    const rate = ref(1.0)
    const volume = ref(1.0)
    const pitch = ref(1.0)
    const currentText = ref('')
    const currentAudio = ref<HTMLAudioElement | null>(null)
    
    // åˆ†æ®µæœ—è¯»ç›¸å…³
    const segments = ref<AudioSegment[]>([])
    const currentSegmentIndex = ref(0)
    const isLoadingSegments = ref(false)

    // æ£€æŸ¥æ˜¯å¦æ”¯æŒ TTS
    const isSupported = computed(() => {
        return 'speechSynthesis' in window
    })

    // æ™ºèƒ½åˆ†æ®µï¼šæŒ‰å¥å­åˆ†å‰²ï¼Œæ¯æ®µæœ€å¤š 500 å­—ç¬¦
    const splitTextIntoSegments = (text: string, maxLength: number = 500): string[] => {
        if (text.length <= maxLength) {
            return [text]
        }

        const segments: string[] = []
        // æŒ‰å¥å­åˆ†å‰²ï¼ˆä¸­æ–‡å¥å·ã€é—®å·ã€æ„Ÿå¹å·ã€è‹±æ–‡å¥å·ç­‰ï¼‰
        const sentences = text.split(/([ã€‚ï¼ï¼Ÿ\.!?ï¼›;])/g)
        
        let currentSegment = ''
        
        for (let i = 0; i < sentences.length; i++) {
            const sentence = sentences[i]
            
            // å¦‚æœå½“å‰æ®µè½åŠ ä¸Šæ–°å¥å­ä¸è¶…è¿‡é™åˆ¶ï¼Œå°±æ·»åŠ 
            if ((currentSegment + sentence).length <= maxLength) {
                currentSegment += sentence
            } else {
                // å¦‚æœå½“å‰æ®µè½ä¸ä¸ºç©ºï¼Œä¿å­˜å®ƒ
                if (currentSegment.trim()) {
                    segments.push(currentSegment.trim())
                }
                // å¼€å§‹æ–°æ®µè½
                currentSegment = sentence
            }
        }
        
        // æ·»åŠ æœ€åä¸€æ®µ
        if (currentSegment.trim()) {
            segments.push(currentSegment.trim())
        }
        
        console.log(`ğŸ“„ æ–‡æœ¬åˆ†æ®µ: æ€»é•¿åº¦ ${text.length} å­—ç¬¦ï¼Œåˆ†ä¸º ${segments.length} æ®µ`)
        return segments
    }

    // åŠ è½½å¯ç”¨è¯­éŸ³åˆ—è¡¨
    const loadVoices = () => {
        if (!isSupported.value) return

        const loadVoiceList = () => {
            voices.value = window.speechSynthesis.getVoices()

            // å°è¯•é€‰æ‹©ä¸­æ–‡è¯­éŸ³ä½œä¸ºé»˜è®¤
            const zhVoice = voices.value.find(v =>
                v.lang.startsWith('zh') || v.lang.includes('Chinese')
            )
            if (zhVoice && !selectedVoice.value) {
                selectedVoice.value = zhVoice
            }

            console.log('ğŸ”Š å¯ç”¨è¯­éŸ³æ•°é‡:', voices.value.length)
        }

        // Chrome éœ€è¦ç›‘å¬ voiceschanged äº‹ä»¶
        if (window.speechSynthesis.onvoiceschanged !== undefined) {
            window.speechSynthesis.onvoiceschanged = loadVoiceList
        }

        loadVoiceList()
    }

    // åŠ è½½ Edge TTS è¯­éŸ³åˆ—è¡¨
    const loadEdgeVoices = async () => {
        try {
            const result = await ttsAPI.getVoices()
            edgeVoices.value = result.chinese
            console.log('ğŸ”Š Edge TTS ä¸­æ–‡è¯­éŸ³æ•°é‡:', edgeVoices.value.length)
        } catch (error) {
            console.error('âŒ åŠ è½½ Edge TTS è¯­éŸ³å¤±è´¥:', error)
        }
    }

    // æœ—è¯»æ–‡æœ¬
    const speak = async (text: string, options?: TTSOptions) => {
        console.log('ğŸ¯ [useTextToSpeech] speak æ–¹æ³•è¢«è°ƒç”¨')
        console.log('ğŸ¯ [useTextToSpeech] text é•¿åº¦:', text?.length)
        console.log('ğŸ¯ [useTextToSpeech] text å‰50å­—:', text?.substring(0, 50))
        console.log('ğŸ¯ [useTextToSpeech] options:', options)
        console.log('ğŸ¯ [useTextToSpeech] engine:', options?.engine || engine.value)
        
        if (!text.trim()) {
            console.warn('âš ï¸ [useTextToSpeech] æ–‡æœ¬ä¸ºç©ºï¼Œè·³è¿‡æœ—è¯»')
            return
        }

        // åœæ­¢å½“å‰æœ—è¯»
        stop()

        currentText.value = text
        const selectedEngine = options?.engine || engine.value
        
        console.log('ğŸ¯ [useTextToSpeech] ä½¿ç”¨å¼•æ“:', selectedEngine)

        if (selectedEngine === 'edge') {
            console.log('ğŸ¯ [useTextToSpeech] è°ƒç”¨ speakWithEdge')
            await speakWithEdge(text, options)
        } else if (selectedEngine === 'piper') {
            console.log('ğŸ¯ [useTextToSpeech] è°ƒç”¨ speakWithPiper')
            await speakWithPiper(text, options)
        } else {
            console.log('ğŸ¯ [useTextToSpeech] è°ƒç”¨ speakWithBrowser')
            speakWithBrowser(text, options)
        }
    }

    // ä½¿ç”¨æµè§ˆå™¨ TTS
    const speakWithBrowser = (text: string, options?: TTSOptions) => {
        if (!isSupported.value) return

        const utterance = new SpeechSynthesisUtterance(text)

        // åº”ç”¨è®¾ç½®
        utterance.lang = options?.lang || 'zh-CN'
        utterance.rate = options?.rate ?? rate.value
        utterance.volume = options?.volume ?? volume.value
        utterance.pitch = options?.pitch ?? pitch.value

        if (selectedVoice.value) {
            utterance.voice = selectedVoice.value
        }

        // äº‹ä»¶å¤„ç†
        utterance.onstart = () => {
            isPlaying.value = true
            isPaused.value = false
            console.log('ğŸ”Š å¼€å§‹æœ—è¯» (æµè§ˆå™¨)')
        }

        utterance.onend = () => {
            isPlaying.value = false
            isPaused.value = false
            console.log('ğŸ”Š æœ—è¯»ç»“æŸ')
        }

        utterance.onerror = (event) => {
            console.error('ğŸ”Š æœ—è¯»é”™è¯¯:', event.error)
            isPlaying.value = false
            isPaused.value = false
        }

        utterance.onpause = () => {
            isPaused.value = true
        }

        utterance.onresume = () => {
            isPaused.value = false
        }

        window.speechSynthesis.speak(utterance)
    }

    // ä½¿ç”¨ Edge TTS
    const speakWithEdge = async (text: string, options?: TTSOptions) => {
        try {
            console.log('ğŸ”Š å¼€å§‹ Edge TTS åˆ†æ®µæœ—è¯»')
            isPlaying.value = true
            isPaused.value = false
            isLoadingSegments.value = true

            // åˆ†æ®µ
            const textSegments = splitTextIntoSegments(text, 500)
            segments.value = textSegments.map(text => ({
                text,
                isLoading: false
            }))
            currentSegmentIndex.value = 0

            // é¢„åŠ è½½å‰ 3 æ®µ
            await preloadSegments(0, Math.min(3, segments.value.length), options)
            
            isLoadingSegments.value = false

            // å¼€å§‹æ’­æ”¾ç¬¬ä¸€æ®µ
            await playSegment(0, options)
        } catch (error) {
            console.error('âŒ Edge TTS æœ—è¯»å¤±è´¥:', error)
            isPlaying.value = false
            isPaused.value = false
            isLoadingSegments.value = false
        }
    }

    // é¢„åŠ è½½éŸ³é¢‘ç‰‡æ®µ
    const preloadSegments = async (startIndex: number, endIndex: number, options?: TTSOptions) => {
        const promises = []
        
        for (let i = startIndex; i < endIndex && i < segments.value.length; i++) {
            const segment = segments.value[i]
            
            // è·³è¿‡å·²åŠ è½½æˆ–æ­£åœ¨åŠ è½½çš„ç‰‡æ®µ
            if (segment.audioBlob || segment.isLoading) {
                continue
            }
            
            segment.isLoading = true
            
            const promise = (async () => {
                try {
                    console.log(`ğŸ“¥ é¢„åŠ è½½ç‰‡æ®µ ${i + 1}/${segments.value.length}: "${segment.text.substring(0, 30)}..."`)
                    
                    const audioBlob = await ttsAPI.synthesize(segment.text, {
                        voice: options?.voice || selectedEdgeVoice.value,
                        rate: Math.round(((options?.rate ?? rate.value) - 1) * 100),
                        pitch: Math.round(((options?.pitch ?? pitch.value) - 1) * 50),
                        volume: Math.round(((options?.volume ?? volume.value) - 1) * 50)
                    })
                    
                    segment.audioBlob = audioBlob
                    segment.audioUrl = URL.createObjectURL(audioBlob)
                    segment.isLoading = false
                    
                    console.log(`âœ… ç‰‡æ®µ ${i + 1} åŠ è½½å®Œæˆ`)
                } catch (error) {
                    console.error(`âŒ ç‰‡æ®µ ${i + 1} åŠ è½½å¤±è´¥:`, error)
                    segment.error = error instanceof Error ? error.message : 'åŠ è½½å¤±è´¥'
                    segment.isLoading = false
                }
            })()
            
            promises.push(promise)
        }
        
        await Promise.all(promises)
    }

    // æ’­æ”¾æŒ‡å®šç‰‡æ®µ
    const playSegment = async (index: number, options?: TTSOptions) => {
        if (index >= segments.value.length) {
            // æ‰€æœ‰ç‰‡æ®µæ’­æ”¾å®Œæ¯•
            console.log('ğŸ‰ æ‰€æœ‰ç‰‡æ®µæ’­æ”¾å®Œæ¯•')
            isPlaying.value = false
            isPaused.value = false
            cleanupSegments()
            return
        }

        const segment = segments.value[index]
        currentSegmentIndex.value = index

        // å¦‚æœç‰‡æ®µè¿˜åœ¨åŠ è½½ï¼Œç­‰å¾…
        if (segment.isLoading) {
            console.log(`â³ ç­‰å¾…ç‰‡æ®µ ${index + 1} åŠ è½½...`)
            await new Promise(resolve => {
                const checkInterval = setInterval(() => {
                    if (!segment.isLoading) {
                        clearInterval(checkInterval)
                        resolve(null)
                    }
                }, 100)
            })
        }

        // å¦‚æœåŠ è½½å¤±è´¥ï¼Œè·³è¿‡è¿™ä¸€æ®µ
        if (segment.error || !segment.audioUrl) {
            console.warn(`âš ï¸ è·³è¿‡å¤±è´¥çš„ç‰‡æ®µ ${index + 1}`)
            await playSegment(index + 1, options)
            return
        }

        try {
            console.log(`â–¶ï¸ æ’­æ”¾ç‰‡æ®µ ${index + 1}/${segments.value.length}`)
            
            // åˆ›å»ºéŸ³é¢‘å¯¹è±¡
            const audio = new Audio(segment.audioUrl)
            currentAudio.value = audio

            // éŸ³é¢‘æ’­æ”¾ç»“æŸåï¼Œæ’­æ”¾ä¸‹ä¸€æ®µ
            audio.onended = async () => {
                console.log(`âœ… ç‰‡æ®µ ${index + 1} æ’­æ”¾å®Œæˆ`)
                
                // é¢„åŠ è½½åç»­ç‰‡æ®µ
                const nextPreloadIndex = index + 3
                if (nextPreloadIndex < segments.value.length) {
                    preloadSegments(nextPreloadIndex, nextPreloadIndex + 1, options)
                }
                
                // æ’­æ”¾ä¸‹ä¸€æ®µ
                await playSegment(index + 1, options)
            }

            audio.onerror = (error) => {
                console.error(`âŒ ç‰‡æ®µ ${index + 1} æ’­æ”¾é”™è¯¯:`, error)
                // å°è¯•æ’­æ”¾ä¸‹ä¸€æ®µ
                playSegment(index + 1, options)
            }

            // æ’­æ”¾éŸ³é¢‘
            await audio.play()
        } catch (error) {
            console.error(`âŒ æ’­æ”¾ç‰‡æ®µ ${index + 1} å¤±è´¥:`, error)
            // å°è¯•æ’­æ”¾ä¸‹ä¸€æ®µ
            await playSegment(index + 1, options)
        }
    }

    // æ¸…ç†ç‰‡æ®µç¼“å­˜
    const cleanupSegments = () => {
        segments.value.forEach(segment => {
            if (segment.audioUrl) {
                URL.revokeObjectURL(segment.audioUrl)
            }
        })
        segments.value = []
        currentSegmentIndex.value = 0
    }

    // ä½¿ç”¨ Piper TTS (é¢„ç•™æ¥å£)
    const speakWithPiper = async (text: string, options?: TTSOptions) => {
        console.warn('âš ï¸ Piper TTS å°šæœªå®ç°')
        // TODO: å®ç° Piper TTS
    }

    // æš‚åœ
    const pause = () => {
        if (engine.value === 'browser' && isSupported.value) {
            window.speechSynthesis.pause()
            isPaused.value = true
        } else if (currentAudio.value) {
            currentAudio.value.pause()
            isPaused.value = true
        }
    }

    // ç»§ç»­
    const resume = () => {
        if (engine.value === 'browser' && isSupported.value) {
            window.speechSynthesis.resume()
            isPaused.value = false
        } else if (currentAudio.value) {
            currentAudio.value.play()
            isPaused.value = false
        }
    }

    // åœæ­¢
    const stop = () => {
        if (engine.value === 'browser' && isSupported.value) {
            window.speechSynthesis.cancel()
        }
        
        if (currentAudio.value) {
            currentAudio.value.pause()
            currentAudio.value.currentTime = 0
            currentAudio.value = null
        }
        
        // æ¸…ç†åˆ†æ®µç¼“å­˜
        cleanupSegments()
        
        isPlaying.value = false
        isPaused.value = false
        currentText.value = ''
        isLoadingSegments.value = false
    }

    // åˆ‡æ¢æ’­æ”¾/æš‚åœ
    const toggle = () => {
        if (isPaused.value) {
            resume()
        } else if (isPlaying.value) {
            pause()
        }
    }

    // è®¾ç½®è¯­é€Ÿ
    const setRate = (newRate: number) => {
        rate.value = Math.max(0.5, Math.min(2, newRate))
    }

    // è®¾ç½®éŸ³é‡
    const setVolume = (newVolume: number) => {
        volume.value = Math.max(0, Math.min(1, newVolume))
    }

    // è®¾ç½®è¯­éŸ³
    const setVoice = (voice: SpeechSynthesisVoice) => {
        selectedVoice.value = voice
        console.log('ğŸ”Š åˆ‡æ¢è¯­éŸ³:', voice.name, voice.lang)
        
        // ğŸ¯ å¦‚æœæ­£åœ¨æœ—è¯»ï¼Œé‡æ–°å¼€å§‹ä»¥åº”ç”¨æ–°è¯­éŸ³
        if (isPlaying.value && currentText.value) {
            const wasPlaying = !isPaused.value
            const textToSpeak = currentText.value
            
            // åœæ­¢å½“å‰æœ—è¯»
            stop()
            
            // ä½¿ç”¨æ–°è¯­éŸ³é‡æ–°æœ—è¯»
            if (wasPlaying) {
                setTimeout(() => {
                    speak(textToSpeak)
                }, 100)
            }
        }
    }

    // è®¾ç½® Edge è¯­éŸ³
    const setEdgeVoice = (voiceName: string) => {
        selectedEdgeVoice.value = voiceName
        console.log('ğŸ”Š åˆ‡æ¢ Edge è¯­éŸ³:', voiceName)
    }

    // è®¾ç½® TTS å¼•æ“
    const setEngine = (newEngine: TTSEngine) => {
        engine.value = newEngine
        console.log('ğŸ”Š åˆ‡æ¢ TTS å¼•æ“:', newEngine)
    }

    // è·å–ä¸­æ–‡è¯­éŸ³åˆ—è¡¨
    const chineseVoices = computed(() => {
        return voices.value.filter(v =>
            v.lang.startsWith('zh') || v.lang.includes('Chinese')
        )
    })

    // åˆå§‹åŒ–
    onMounted(() => {
        loadVoices()
        loadEdgeVoices()
    })

    // æ¸…ç†
    onUnmounted(() => {
        stop()
    })

    return {
        // çŠ¶æ€
        isSupported,
        isPlaying,
        isPaused,
        voices,
        edgeVoices,
        chineseVoices,
        selectedVoice,
        selectedEdgeVoice,
        engine,
        rate,
        volume,
        pitch,
        currentText,
        // åˆ†æ®µæœ—è¯»çŠ¶æ€
        segments,
        currentSegmentIndex,
        isLoadingSegments,

        // æ–¹æ³•
        speak,
        pause,
        resume,
        stop,
        toggle,
        setRate,
        setVolume,
        setVoice,
        setEdgeVoice,
        setEngine,
        loadVoices,
        loadEdgeVoices
    }
}
